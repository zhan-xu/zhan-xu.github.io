<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zhan's personal page</title>
  <style>
    /* Basic Reset */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background-color: #e9f2f9; /* Light, calming background */
      color: #333;
      line-height: 1.6;
    }
    header {
      background: #005f73; /* A deep teal */
      color: #fff;
      padding: 40px 10px;
      text-align: center;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      display: none; /* Removed the header as requested */
    }
    nav {
      display: none; /* Hide the navigation bar */
    }
    .container {
      max-width: 1000px;
      margin: 30px auto;
      background: #ffffff;
      padding: 30px;
      border-radius: 12px;
      box-shadow: 0 8px 20px rgba(0,0,0,0.08);
    }
    .section-title {
      font-size: 2em;
      color: #005f73;
      border-bottom: 3px solid #0096c7; /* A bright, contrasting blue */
      padding-bottom: 10px;
      margin-top: 30px;
      margin-bottom: 20px;
      font-weight: 500;
      display: inline-block; /* Allow the link to be on the same line */
    }
    .section-title-wrapper {
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      flex-wrap: wrap;
    }
    .section-title-wrapper a {
      font-size: 1em;
      text-decoration: none;
      color: #0077b6;
      font-weight: 500;
    }
    .section-title-wrapper a:hover {
      text-decoration: underline;
    }
    /* About Section Layout */
    .about-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      text-align: center;
      gap: 20px;
      margin-bottom: 40px;
      padding: 40px 20px; /* Add padding to the top to replace the header space */
      border-radius: 12px;
      background: #005f73;
      color: #fff;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .about-header {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 20px;
    }
    .about-photo img {
      width: 200px;
      height: 200px;
      object-fit: cover;
      border-radius: 50%; /* Make the photo round */
      border: 4px solid #fff;
      box-shadow: 0 0 15px rgba(0,0,0,0.1);
    }
    .about-info {
      text-align: center;
      max-width: 700px;
    }
    .about-info h1 {
      font-size: 3em;
      margin-bottom: 5px;
      font-weight: 600;
    }
    .about-info h2 {
      font-size: 1.3em;
      font-weight: 300;
      color: #fff;
    }
    .about-info p {
      font-size: 1.1em;
      color: #fff;
      margin-bottom: 15px;
      font-weight: 300;
    }
    .about-info a {
      color: #89c2d9; /* A light blue for links in the dark header section */
      text-decoration: none;
      font-weight: 600;
    }
    .about-info a:hover {
      text-decoration: underline;
    }
    /* Publications styling */
    .publications-list {
      display: flex;
      flex-direction: column;
      gap: 20px; /* Spacing between publication items */
    }
    .publication {
      display: flex;
      align-items: flex-start;
      margin-bottom: 0; /* Removed bottom margin as gap handles spacing */
      border-bottom: 1px solid #ddd;
      padding-bottom: 15px;
    }
    .publication:last-child {
      border-bottom: none; /* Remove border from the last item */
    }
    .publication img, .publication video {
      width: 250px;
      height: auto;
      margin-right: 20px;
      border-radius: 5px;
      border: 1px solid #ccc;
    }
    .publication .info {
      flex: 1;
    }
    .publication .info h3 {
      font-size: 1.4em;
      margin-bottom: 5px;
      color: #005f73;
    }
    .publication .info h3 a {
      color: #005f73;
      text-decoration: none;
      transition: color 0.3s ease;
    }
    .publication .info h3 a:hover {
      color: #0096c7;
    }
    .publication .info p {
      margin-bottom: 5px;
      font-size: 1em;
      color: #666;
    }
    .publication .info p a {
      color: #0077b6;
      text-decoration: none;
    }
    .publication .info p a:hover {
      text-decoration: underline;
    }
    .publication .links {
      margin-top: 10px;
      display: flex;
      gap: 15px;
    }
    .publication .links a {
      color: #0077b6;
      text-decoration: none;
      font-weight: 600;
      transition: color 0.3s ease;
      /* Remove fixed width and centering for a more compact, natural fit */
    }
    .publication .links a:hover {
      color: #0096c7;
    }
    /* Responsive design */
    @media (max-width: 768px) {
      .about-container {
        flex-direction: column;
        align-items: center;
      }
      .about-header {
        flex-direction: column;
      }
      .publication {
        flex-direction: column;
        align-items: center;
        text-align: center;
      }
      .publication img, .publication video {
        width: 100%;
        max-width: 300px;
        margin-right: 0;
        margin-bottom: 15px;
      }
      .publication .info {
        align-items: center;
        text-align: center;
      }
      .publication .links a {
        margin: 0 8px;
      }
    }
    @media (min-width: 769px) {
      .about-header {
        flex-direction: row;
        gap: 40px;
      }
    }
  </style>
</head>
<body>

  <header></header>
  <nav></nav>

  <div class="container">
    <div id="about" class="about-container">
      <div class="about-header">
        <div class="about-photo">
          <img src="profile.jpg" alt="Profile Photo">
        </div>
        <div class="about-info">
          <!-- <h1>Zhan Xu</h1> -->
          <!-- <h2>Research Scientist | Adobe Research</h2> -->
          <p>I'm Zhan Xu. I was a research scientist at <a href="https://research.adobe.com/">Adobe Research</a> from Feb. 2023 to Sep. 2025. I received my Ph.D. in Feb. 2023 from the <a href="https://www.umass.edu/">University of Massachusetts Amherst</a>, where I was advised by Prof. <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>.</p>
          <p>My research focuses on 2D/3D character animation and generative video effects. You can reach me by email at: <a href="mailto:zhanxu@cs.umass.edu">zhanxu AT cs DOT umass DOT edu</a>.</p>
        </div>
      </div>
    </div>

    <div id="publications">
      <div class="section-title-wrapper">
        <h2 class="section-title">Publications</h2>
        <a href="https://scholar.google.com/citations?user=pF2vMhgAAAAJ&hl=zh-CN">For a complete list, see my Google Scholar page.</a>
      </div>



        <div class="publication">
          <img src="publication/video_motion_graph.png" alt="Video Motion Graphs snapshot">
          <div class="info">
            <h3><a href="https://h-liu1997.github.io/Video-Motion-Graphs/"><b>Video Motion Graphs</b></a></h3>
            <p><a href="https://h-liu1997.github.io/">Haiyang Liu</a>, <strong>Zhan Xu</strong>, <a href="https://harlanhong.github.io/">Fa-Ting Hong</a>, <a href="https://hhsinping.github.io/">Hsin-Ping Huang</a>, <a href="https://zhouyisjtu.github.io/">Yi Zhou</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a></p>
            <p>International Conference on Computer Vision (ICCV), 2025, Highlight</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2503.20218">PAPER</a>]
            </div>
          </div>
        </div>

      
        <div class="publication">
          <video src="publication/vins.mp4" style="width: 250px; height: auto;" autoplay="" loop="" muted="" playsinline=""></video>
          <div class="info">
            <h3><a href="https://glitchinthematrix.github.io/vins/"><b>Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks</b></a></h3>
            <p><a href="https://bhishmadedhia.com/">Bhishma Dedhia</a>, <a href="https://www.davidbourgin.com/">David Bourgin</a>, <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>, <a href="https://yuheng-li.github.io/">Yuheng Li</a>, <a href="https://research.adobe.com/person/yan-kang/">Yan Kang</a>, <strong>Zhan Xu</strong>, <a href="https://www.princeton.edu/~jha/">Niraj K Jha</a>, <a href="https://lychenyoko.github.io/">Yuchen Liu</a></p>
            <p>International Conference on Computer Vision (ICCV), 2025</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2507.13966">PAPER</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/rig-anything.gif" alt="RigAnything snapshot">
          <div class="info">
            <h3><a href="https://www.liuisabella.com/RigAnything/"><b>RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets</b></a></h3>
            <p><a href="https://www.liuisabella.com/">Isabella Liu</a>, <strong>Zhan Xu</strong>, <a href="https://yifita.netlify.app/">Wang Yifan</a>, <a href="https://www.cs.unc.edu/~airsplay/">Hao Tan</a>, <a href="https://zexiangxu.github.io/">Zexiang Xu</a>, <a href="https://xiaolongw.github.io/">Xiaolong Wang</a>, <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a>, <a href="https://vivianszf.github.io/">Zifan Shi</a></p>
            <p>ACM Transactions on Graphics (SIGGRAPH), 2025</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2502.09615">PAPER</a>]
              [<a href="https://www.liuisabella.com/RigAnything/#full_video">VIDEO</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/streamme.gif" alt="StreamME snapshot">
          <div class="info">
            <h3><a href="https://songluchuan.github.io/StreamME/"><b>StreamME: Simplify 3D Gaussian Avatar within Live Stream</b></a></h3>
            <p><a href="https://songluchuan.github.io/">Luchuan Song</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <strong>Zhan Xu</strong>, <a href="https://zhouyisjtu.github.io/">Yi Zhou</a>, <a href="https://research.adobe.com/person/deepali-aneja/">Deepali Aneja</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a></p>
            <p>Proceedings of SIGGRAPH, 2025</p>
            <div class="links">
              [<a href="https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/SIGGRAPHCONFERENCEPAPERS25/46/33c12f4a-1b20-11f0-ada9-16bb50361d1f/OUT/siggraphconferencepapers25-46.html">PAPER</a>]
              [<a href="https://songluchuan.github.io/StreamME/">CODE</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/move.gif" alt="Move-in-2D snapshot">
          <div class="info">
            <h3><a href="https://hhsinping.github.io/Move-in-2D/"><b>Move-in-2D: 2D-Conditioned Human Motion Generation</b></a></h3>
            <p><a href="https://hhsinping.github.io/">Hsin-Ping Huang</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="http://juiwang.com/">Jui-Hsien Wang</a>, <a href="https://difanliu.github.io/">Difan Liu</a>, <a href="https://pages.cs.wisc.edu/~fliu/">Feng Liu</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, <strong>Zhan Xu</strong></p>
            <p>Proceedings of the Computer Vision and Pattern Recognition (CVPR), 2025</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2412.13185">PAPER</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/freeview.png" alt="Free-viewpoint snapshot">
          <div class="info">
            <h3><a href="https://harlanhong.github.io/publications/fvhuman/index.html"><b>Free-viewpoint Human Animation with Pose-correlated Reference Selection</b></a></h3>
            <p><a href="https://harlanhong.github.io/">Fa-Ting Hong</a>, <strong>Zhan Xu</strong>, <a href="https://h-liu1997.github.io/">Haiyang Liu, <a href="https://qinjielin-nu.github.io/">Qinjie Lin</a>, <a href="https://songluchuan.github.io/">Luchuan Song</a>, <a href="https://zhixinshu.github.io/">Zhixin Shu</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="https://www.duygu-ceylan.com/">Duygu Ceylan</a>, <a href="https://www.danxurgb.net/">Dan Xu</a></p>
            <p>Proceedings of the Computer Vision and Pattern Recognition (CVPR), 2025, Highlight</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2412.17290">PAPER</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/persona.png" alt="Visual Persona snapshot">
          <div class="info">
            <h3><a href="https://cvlab-kaist.github.io/Visual-Persona/"><b>Visual Persona: Foundation Model for Full-Body Human Customization</b></a></h3>
            <p><a href="https://nam-jisu.github.io/">Jisu Nam</a>, <a href="https://scholar.google.com/citations?user=Eo87mRsAAAAJ&hl=ko&oi=ao">Soowon Son</a>, <strong>Zhan Xu</strong>, <a href="https://jshi31.github.io/jingshi/">Jing Shi</a>, <a href="https://difanliu.github.io/">Difan Liu</a>, <a href="https://pages.cs.wisc.edu/~fliu/">Feng Liu</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=WEVBTxsAAAAJ">Aashish Misraa</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=cIK1hS8AAAAJ">Seungryong Kim</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a></p>
            <p>Proceedings of the Computer Vision and Pattern Recognition (CVPR), 2025</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2503.15406">PAPER</a>]
              [<a href="https://github.com/cvlab-kaist/Visual-Persona">CODE</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/actanywhere.png" alt="ActAnywhere snapshot">
          <div class="info">
            <h3><a href="https://actanywhere.github.io/"><b>ActAnywhere: Subject-Aware Video Background Generation</b></a></h3>
            <p><a href="https://cs.stanford.edu/~bxpan/">Boxiao Pan</a>, <strong>Zhan Xu</strong>, <a href="https://paulchhuang.wixsite.com/chhuang">Chun-Hao Paul Huang, <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>, <a href="https://jimeiyang.github.io/">Jimei Yang</a></p>
            <p>Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2401.10822">PAPER</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/neca.jpg" alt="NECA snapshot">
          <div class="info">
            <h3><b>NECA: Neural Customizable Human Avatar</b></h3>
            <p>Junjin Xiao, <a href="https://www.zhangqing-home.net/">Qing Zhang</a>, <strong>Zhan Xu</strong>, <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng</a>
            <p>Proceedings of the Computer Vision and Pattern Recognition (CVPR), 2024</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2403.10335">PAPER</a>]
              [<a href="https://github.com/iSEE-Laboratory/NECA">CODE</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/morig.jpg" alt="MoRig snapshot">
          <div class="info">
            <h3><a href="https://zhan-xu.github.io/motion-rig"><b>MoRig: Motion-Aware Rigging of Character Meshes from Point Clouds</b></a></h3>
            <p><strong>Zhan Xu</strong>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="https://ericyi.github.io/">Li Yi</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a></p>
            <p>Proceedings of SIGGRAPH ASIA, 2022</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2210.09463">PAPER</a>]
              [<a href="https://github.com/zhan-xu/MoRig">CODE</a>]
              [<a href="https://www.youtube.com/watch?v=xNcw5UtpNrc">VIDEO</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/apes.png" alt="APES snapshot">
          <div class="info">
            <h3><a href="https://zhan-xu.github.io/parts"><b>APES: Articulated Part Extraction from Sprite Sheets</b></a></h3>
            <p><strong>Zhan Xu</strong>, <a href="https://techmatt.github.io/">Matthew Fisher</a>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="https://research.adobe.com/person/deepali-aneja/">Deepali Aneja</a>, <a href="https://www.linkedin.com/in/rushikesh-dudhat-274245147/">Rushikesh Dudhat</a>, <a href="https://ericyi.github.io/">Li Yi</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a></p>
            <p>Proceedings of the Computer Vision and Pattern Recognition (CVPR), 2022</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2206.02015">PAPER</a>]
              [<a href="https://github.com/zhan-xu/APES">CODE</a>]
              [<a href="https://www.youtube.com/watch?v=XCf68SKsaRk">VIDEO</a>]
            </div>
          </div>
        </div>
        
        <div class="publication">
          <img src="publication/rignet.png" alt="RigNet snapshot">
          <div class="info">
            <h3><a href="https://zhan-xu.github.io/rig-net"><b>RigNet: Neural Rigging for Articulated Characters</b></a> </h3>
            <p><strong>Zhan Xu</strong>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>, <a href="https://www.chrislandreth.com/">Chris Landreth</a>, <a href="https://www.dgp.toronto.edu/~karan/">Karan Singh</a></p>
            <p>ACM Transactions on Graphics (SIGGRAPH), 2020</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/2005.00559">PAPER</a>]
              [<a href="https://github.com/zhan-xu/RigNet">CODE</a>]
              [<a href="https://www.youtube.com/watch?v=J90VETgWIDg">VIDEO</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/volskelnet.jpg" alt="Predicting Animation Skeletons snapshot">
          <div class="info">
            <h3><a href="https://people.cs.umass.edu/%7Ezhanxu/projects/AnimSkelVolNet/"><b>Predicting Animation Skeletons for 3D Articulated Models via Volumetric Nets</b></a></h3>
            <p><strong>Zhan Xu</strong>, <a href="https://yzhou359.github.io/">Yang Zhou</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>, <a href="https://www.dgp.toronto.edu/~karan/">Karan Singh</a></p>
            <p>International Conference on 3D Vision (3DV), 2019, Oral Presentation</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/1908.08506">PAPER</a>]
              [<a href="https://github.com/zhan-xu/AnimSkelVolNet">CODE</a>]
            </div>
          </div>
        </div>

        <div class="publication">
          <img src="publication/visemenet.png" alt="VisemeNet snapshot">
          <div class="info">
            <h3><a href="https://people.umass.edu/~yangzhou/visemenet/"><b>VisemeNet: Audio-Driven Animator-Centric Speech Animation</b></a></h3>
            <p><a href="https://yzhou359.github.io/">Yang Zhou</a>, <strong>Zhan Xu</strong>, <a href="https://www.chrislandreth.com/">Chris Landreth</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>, <a href="https://people.cs.umass.edu/~smaji/">Subhransu Maji</a>, <a href="https://www.dgp.toronto.edu/~karan/">Karan Singh</a></p>
            <p>ACM Transactions on Graphics (SIGGRAPH), 2018</p>
            <div class="links">
              [<a href="https://arxiv.org/abs/1805.09488">PAPER</a>]
              [<a href="https://github.com/yzhou359/VisemeNet_tensorflow">CODE</a>]
              [<a href="https://www.youtube.com/watch?v=kk2EnyMD3mo">VIDEO</a>]
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>

</body>
</html>
